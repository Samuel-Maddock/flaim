import itertools
from typing import List
from collections import defaultdict
import numpy as np
import os

from synth_fl.generators import Generator, GeneratorConfig
from synth_fl.libs.private_pgm.mbi import FactoredInference, GraphicalModel
from synth_fl.utils import logger, DEFAULT_DATA_ROOT
from synth_fl.utils.dataloaders import TabularDataset
from synth_fl.utils.privacy import (
    ExponentialMechanism,
    GaussianMechanism,
    LaplaceMechanism,
    zCDPAccountant,
    AutoDPAccountant,
)


from typing import Tuple

# Code refactored from synth_fl/libs/private_pgm/mechanisms/aim.py


class AIM(Generator):
    def __init__(
        self,
        config: GeneratorConfig,
        disable_cdp_init=False,
        subsample_rate=1,
    ) -> None:
        super().__init__(config)
        self.name = "aim"
        # Initialise accountant and mechanisms
        # Used to improve init performance in FLAIM
        rho = 0 if disable_cdp_init else None
        self.accountant_type = self.config.accountant_type
        self.subsample_rate = subsample_rate  # no subsampling

        if self.accountant_type == "zcdp":
            self.accountant = zCDPAccountant(
                self.config.epsilon, self.config.delta, rho
            )
        else:
            self.accountant = AutoDPAccountant(
                self.config.epsilon,
                self.config.delta,
                sample_rate=self.subsample_rate,
            )

        if config.aim_mech == "gaussian":
            self.gauss_mech = GaussianMechanism(self.accountant)
        else:
            self.gauss_mech = LaplaceMechanism(self.accountant)
        self.exp_mech = ExponentialMechanism(self.accountant)

        self.init_gauss_sigma, self.init_exp_epsilon = None, None
        self.anneal_type = config.anneal_type
        self.model = None  # PGM Model
        self.batch_queries = False

        self.workload_type = config.workload_type
        self.workload_closure = None  # original closure is stored here

        self.pgm_train_iters = config.pgm_train_iters
        self.pgm_final_iters = config.pgm_final_iters
        self.pgm_weight_method = config.pgm_weight_method
        self.global_rounds = config.global_rounds
        self.adaptive_noise = True if not self.global_rounds else False

        self.gauss_budget_alloc = config.gauss_budget_alloc
        self.exp_budget_alloc = 1 - self.gauss_budget_alloc
        self.selection_method = config.selection_method
        if self.gauss_budget_alloc == 1:
            logger.debug(
                f"Gaussian budget alloc is 1, forcing selection to be random instead of exp mech"
            )
            self.selection_method = "server_random"
        if "random" in self.selection_method:
            logger.debug(
                f"Selection method is random (not exp mech), forcing budget alloc=(1,0)"
            )
            self.gauss_budget_alloc = 1
            self.exp_budget_alloc = 0
        if self.selection_method == "none":
            logger.debug(
                f"Selection method is 'none', setting global rounds=0 - model init only"
            )
            self.global_rounds = 0
            self.gauss_budget_alloc = 1
            self.exp_budget_alloc = 0

        # Override to change num of synth data rows generated by PGM
        self.synth_rows = None

        # Attributes used by child classes
        self.rounds = None
        self.num_oneways = None
        self.n = None
        self.normalize_measurements = config.normalize_measurements

        self.decision_map = defaultdict(list)
        self.log_decisions = config.log_decisions
        self.client_answers_dtype = "uint16"  # todo

        if self.backend_model == "rap":
            self.normalize_measurements = True

        # RAPConfig args
        self.categorical_consistency = True
        self.n_prime = 1000
        # self.sgd_iters = 5000  # 1000 # SGD iters
        self.sgd_iters = 50
        self.learning_rate = 1e-3
        self.lambda_l1 = 0
        self.k = 3
        self.top_q = 1
        self.stopping_condition = 10**-7
        self.initialize_binomial = False
        self.norm = "L2"

        self.internal_metrics["actual_rounds"] = 0

    # Workload helpers
    def _powerset(self, iterable):
        "powerset([1,2,3]) --> (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)"
        s = list(iterable)
        return itertools.chain.from_iterable(
            itertools.combinations(s, r) for r in range(1, len(s) + 1)
        )

    def _downward_closure(self, Ws):
        ans = set()
        for proj in Ws:
            ans.update(self._powerset(proj))
        if self.workload_type == "target_closure":
            ans = list(filter(lambda x: self.target in x or len(x) == 1, ans))
        return list(ans)
        # return list(sorted(ans, key=len))

    def _compile_workload(self, workload):
        def score(cl):
            return sum(len(set(cl) & set(ax)) for ax in workload)

        return {cl: score(cl) for cl in self._downward_closure(workload)}

    def _hypothetical_model_size(self, domain, cliques):
        # Suppress mp ordering etc.
        model = GraphicalModel(domain, cliques, size_only=True)
        return model.size * 8 / 2**20

    def _filter_candidates(self, candidates, model, size_limit):
        # todo: rap backend doesn't filter marginals by size
        new_candidates = {}
        if self.backend_model == "rap":
            # return candidates
            for cl in candidates:
                if cl not in model.queries_used:
                    new_candidates[cl] = candidates[cl]
            return new_candidates

        free_cliques = self._downward_closure(model.cliques)
        for cl in candidates:
            cond1 = (
                self._hypothetical_model_size(model.domain, model.cliques + [cl])
                <= size_limit
            )
            cond2 = cl in free_cliques
            if cond1 or cond2:
                new_candidates[cl] = candidates[cl]
        logger.debug(
            f"Filtering candidates - {len(candidates)} to {len(new_candidates)}"
        )
        return new_candidates

    def _log_decisions(self, t, gauss_sigma, cl):
        if self.log_decisions:
            self.decision_map[cl].append((t, gauss_sigma))

    def _normalise_measurement(self, measurement, n=None):
        if np.sum(measurement) == 0:
            logger.warning(f"Zero measurement detected in _normalise_measurement() ...")
            return measurement
        n = n if n else 1
        return n * (measurement.astype("float32") / measurement.sum())

    def _gauss_query(
        self,
        client_answers,
        queries,
        gauss_sigma,
        t=0,
        disable_weighting=False,
        batch_query_ans=None,
    ):
        measurements = []
        sensitivity = 1
        for i, cl in enumerate(queries):
            query_ans = (
                batch_query_ans[i]
                if batch_query_ans is not None
                else sum(answers[cl] for answers in client_answers)
            )
            if gauss_sigma > 0:
                y = self.gauss_mech.apply(
                    query_ans, gauss_sigma, sensitivity=sensitivity
                )
                n_est = y.sum()
                y = self._normalise_measurement(y, n=self.n)
            else:
                y = query_ans
                n_est = y.sum()

            self._log_decisions(t, gauss_sigma, cl)
            weight = gauss_sigma
            if not disable_weighting:
                if "scaled" in self.pgm_weight_method:
                    weight *= 1 / query_ans.sum()
                if "n_est" in self.pgm_weight_method:
                    weight = gauss_sigma / n_est
                if "nweight" in self.pgm_weight_method:
                    weight *= self.n
            logger.debug(f"PGM weight - sigma={gauss_sigma}, weight={weight}")
            measurements.append([None, y, weight, cl])

        return measurements

    def _compute_quality_score(
        self,
        cl,
        model,
        control,
        client_answers,
        candidates,
        gauss_sigma,
        query_est=None,
        precomputed_query_ans=None,
    ):
        # non-priv ans
        query_ans = (
            precomputed_query_ans
            if precomputed_query_ans is not None
            else sum(answers[cl] for answers in client_answers)
        )

        # model est
        if query_est is None:
            query_est = model.project(cl).datavector()

        # For edge cases when model is poorly initialised
        if np.sum(query_est) == 0:
            # Set uniform, wil be normalised to prob dist anyway
            query_est = np.ones(query_est.size) / query_est.size

        # Weight, bias and control terms
        gauss_sensitivity = 1 / (query_ans.sum())
        wgt = candidates[cl] if self.config.epsilon > 0 else 1
        bias = (
            (
                np.sqrt(2 / np.pi)
                * gauss_sigma
                * gauss_sensitivity
                * model.domain.size(cl)
            )
            if self.config.epsilon > 0
            else 0
        )
        if self.normalize_measurements:
            query_ans = self._normalise_measurement(query_ans, n=1)
            query_est = self._normalise_measurement(query_est, n=1)
        err = np.linalg.norm(query_ans - query_est, 1)
        return wgt * (err - bias + control)

    def _exp_query(
        self,
        client_answers,
        model,
        candidates,
        exp_eps,
        gauss_sigma,
        t=0,
        batch_query_ans=None,
        control_variates=None,
        model_estimates=None,
    ):
        errors = {}
        sensitivity = {}
        sensitivity_constant = 1 if control_variates is None else 2

        if not control_variates:
            control_variates = defaultdict(int)  # zeros if not using control term

        if not model_estimates:
            # model_estimates = model.calculate_many_marginals(candidates)
            # model_estimates = {
            #     cl: fac.values.flatten() for cl, fac in model_estimates.items()
            # }
            model_estimates = {
                cl: model.project(cl).datavector().astype(self.client_answers_dtype)
                for cl in candidates
            }

        for i, cl in enumerate(candidates):
            if i == 0:
                sensitivity_constant *= (
                    1 / sum(answers[cl] for answers in client_answers).sum()
                )
            errors[cl] = self._compute_quality_score(
                cl,
                model,
                control_variates[cl],
                client_answers,
                candidates,
                gauss_sigma,
                query_est=model_estimates[cl],
                precomputed_query_ans=batch_query_ans[i]
                if batch_query_ans is not None
                else None,
            )
            sensitivity[cl] = abs(candidates[cl] if self.config.epsilon > 0 else 1)

        # if all weights are 0, could be a problem
        max_sensitivity = max(sensitivity.values()) * sensitivity_constant
        if self.config.epsilon > 0:
            return self.exp_mech.apply(errors, exp_eps, max_sensitivity)
        else:
            selected = max(errors, key=errors.get)
            return selected

    def _compute_client_answers(
        self,
        horizontal_datasets,
        workload,
        original_dataset,
        save_cache=False,
        load_cache=False,
    ):
        dataset = original_dataset
        workload_type = (
            "target"
            if self.config.workload_type == "target_closure"
            else self.config.workload_type
        )
        CACHED_ANSWER_PATH = f"{DEFAULT_DATA_ROOT}/client_answers_{dataset.name}_p={dataset.p}_wtype={workload_type}_{self.config.workload_num_marginals}_wseed={self.config.workload_seed}_dseed={dataset.random_state}_k={len(horizontal_datasets)}_niid={dataset.non_iid}.npz"
        if load_cache and os.path.exists(CACHED_ANSWER_PATH):
            logger.info(f"Loading cached client answers from {CACHED_ANSWER_PATH}")
            client_answers = np.load(CACHED_ANSWER_PATH, allow_pickle=True)["arr_0"]
        else:
            client_answers = []
            for dataset in horizontal_datasets:
                answer_dict = {}
                for cl in workload:
                    answer_dict[cl] = dataset.fast_project_flatten(cl).astype(
                        self.client_answers_dtype
                    )
                client_answers.append(answer_dict)
            if save_cache:
                logger.info(f"Caching client answers to {CACHED_ANSWER_PATH}")
                np.savez_compressed(CACHED_ANSWER_PATH, client_answers)
        return client_answers

    def _initialise_model(
        self,
        dataset: TabularDataset,
        candidates,
        client_answers,
        gauss_sigma,
        warm_start=True,
    ):
        oneway = [cl for cl in candidates if len(cl) == 1]
        logger.debug(f"AIM Model backend - {self.backend_model}")
        logger.debug(f"AIM - Initialising 1D marginals.. num={len(oneway)}")
        measurements = self._gauss_query(
            client_answers,
            oneway,
            gauss_sigma,
            t=0,
            disable_weighting=True,
        )
        logger.debug("AIM - Initialising 1D marginals... complete\n")

        if self.backend_model == "pgm":
            engine = FactoredInference(
                dataset.to_pgm_dataset().domain,
                iters=self.pgm_train_iters,
                warm_start=warm_start,
            )
        else:
            engine = RAPEngine(
                self._get_rap_config(dataset.to_rap_dataset(), candidates),
                candidates,
                dataset.domain,
                self.n,
            )

        model = engine.estimate(measurements, total=self.n)
        return model, measurements, engine

    # Methods that rely on accounting
    def _initialise_noise_params(self, rounds):
        gauss_sigma, exp_epsilon = self.init_gauss_sigma, self.init_exp_epsilon
        gauss_rounds = rounds + self.num_oneways
        exp_rounds = rounds
        logger.info(
            f"Initialising noise params - Gauss rounds {gauss_rounds}, Exp rounds {exp_rounds}, alloc=({self.gauss_budget_alloc}, {self.exp_budget_alloc})"
        )
        if self.config.epsilon > 0:
            if self.accountant_type == "zcdp":
                gauss_sigma = np.sqrt(
                    gauss_rounds / (2 * self.gauss_budget_alloc * self.accountant.rho)
                )
                exp_epsilon = np.sqrt(
                    8 * self.exp_budget_alloc * self.accountant.rho / exp_rounds
                )
            else:
                if not self.init_gauss_sigma and not self.init_exp_epsilon:
                    gauss_sigma = self.gauss_mech.get_rdp_sigma(
                        gauss_rounds,
                        q=self.subsample_rate,
                        epsilon=self.gauss_budget_alloc * self.accountant.epsilon,
                    )
                    exp_epsilon = self.exp_mech.get_rdp_sigma(
                        exp_rounds,
                        q=self.subsample_rate,
                        epsilon=self.exp_budget_alloc * self.accountant.epsilon,
                    )
        else:
            gauss_sigma = 1
            exp_epsilon = float("inf")

        return gauss_sigma, exp_epsilon

    def _calibrate_remaining_budget(self, gauss_sigma, exp_epsilon, t=0, rounds=1):
        if self.adaptive_noise:
            if self.accountant_type == "zcdp":
                gauss_sigma = np.sqrt(
                    rounds / (2 * self.gauss_budget_alloc * self.accountant.rho)
                )
                exp_epsilon = np.sqrt(
                    8 * self.exp_budget_alloc * self.accountant.rho / rounds
                )
            else:
                while self._final_round(gauss_sigma, exp_epsilon, t, verbose=False):
                    gauss_sigma += 0.001 * self.init_gauss_sigma
                    exp_epsilon -= 0.001 * self.init_exp_epsilon

        logger.debug(
            f"Calibrating remaining budget for final round - {gauss_sigma}, {exp_epsilon}"
        )
        return gauss_sigma, exp_epsilon

    def _anneal_budget(self, gauss_sigma, exp_epsilon):
        if self.adaptive_noise:
            if self.accountant_type == "zcdp":
                gauss_sigma /= 2
                exp_epsilon *= 2
            else:  # rdp
                gauss_sigma -= 0.05 * self.init_gauss_sigma
                exp_epsilon += 0.05 * self.init_exp_epsilon
        return gauss_sigma, exp_epsilon

    def _final_round(
        self, gauss_sigma, exp_epsilon, t, gauss_count=1, exp_count=1, verbose=True
    ):
        if verbose:
            starting_budget = (
                self.accountant.starting_rho
                if self.accountant_type == "zcdp"
                else self.accountant.epsilon
            )
            current_budget = (
                self.accountant.rho
                if self.accountant_type == "zcdp"
                else self.accountant.get_eps()
            )
            logger.info(f"AIM Round {t} - Budget {starting_budget}, {current_budget}")

        if self.config.epsilon > 0 and self.adaptive_noise:
            if self.accountant_type == "zcdp":
                return self.accountant.rho < 2 * (
                    (gauss_count * 0.5 / gauss_sigma**2)
                    + (exp_count / 8 * exp_epsilon**2)
                )
            else:  # rdp
                mechanisms = [
                    self.gauss_mech.get_privacy_cost(gauss_sigma)[0],
                    self.exp_mech.get_privacy_cost(exp_epsilon)[0],
                ]
                return self.accountant.halt(
                    mechanism=mechanisms,
                    steps=[gauss_count, exp_count],
                )
        else:
            return t >= self.rounds

    def _get_size_limit(self, t=None):
        MAX_MODEL_SIZE = 80  # max_model_size=80 default
        if self.config.epsilon > 0:
            if self.accountant_type == "zcdp":
                return (
                    MAX_MODEL_SIZE
                    * (self.accountant.starting_rho - self.accountant.rho)
                    / self.accountant.starting_rho
                )
            else:
                return (
                    MAX_MODEL_SIZE
                    * self.accountant.current_epsilon
                    / self.accountant.epsilon
                )
        else:
            return MAX_MODEL_SIZE * t / self.rounds

    def _get_initial_num_rounds(self, domain_size):
        if self.config.epsilon > 0 and self.adaptive_noise:
            return 8 * domain_size  # Default AIM parameter
        else:
            return self.global_rounds

    # Main AIM logic
    def _aim_step(
        self,
        engine,
        model,
        measurements,
        client_answers,
        viable_candidates,
        exp_epsilon,
        gauss_sigma,
        t=1,
        n=None,
        control_variates=None,  # not used by central aim, only FL
        suppress_model_update=False,  # Perform step without updating model
        suppress_estimates=False,  # Whether to suppress estimating answer from model
        suppress_gauss_query=False,  # Sets gauss query sigma to 0, but propagates true sigma to exp mech
        model_estimates=None,  # Precomputed model estimates, used to speed up FLAIM
        random_query=False,  # Disables exp mech, random choice
        cl=None,  # Override exp mech,
        disable_weighting=True,
    ):
        # For classes that use AIM internally for a single step, set's the normalising constant when in fed setting
        if n:
            self.n = n

        # Find worst-query via exp mech
        if random_query:
            new_candidates = list(
                filter(lambda x: len(x) <= 2, list(viable_candidates.keys()))
            )
            cl = np.random.choice(new_candidates)
        elif cl is None:
            cl = self._exp_query(
                client_answers,
                model,
                viable_candidates,
                exp_epsilon,
                gauss_sigma,
                t=t,
                control_variates=control_variates,
                model_estimates=model_estimates,
            )

        # Measure chosen query and add to measurements
        round_sample_size = self.n
        logger.debug(f"Selected query {cl}, sample_size={round_sample_size}")
        gauss_sigma = 0 if suppress_gauss_query else gauss_sigma
        measurements.extend(
            self._gauss_query(
                client_answers,
                [cl],
                gauss_sigma,
                t=t,
                disable_weighting=disable_weighting,
            )
        )

        # Calculate new model and query improvement
        prev_model_est, new_model_est = None, None
        if not suppress_estimates:
            prev_model_est = model.project(cl).datavector()

        # In FL settings, the last local model update is suppressed to save computation
        if not suppress_model_update:
            model = engine.estimate(measurements, total=self.n)
            if not suppress_estimates:
                new_model_est = model.project(cl).datavector()

        return model, prev_model_est, new_model_est, cl, measurements, round_sample_size

    # Generator methods
    def _generate(self, horizontal_datasets, workload):
        # Convert datasets to PGM datasets to use .project()
        original_dataset = horizontal_datasets[0]

        horizontal_datasets = [
            dataset.to_pgm_dataset() for dataset in horizontal_datasets
        ]

        # PGMDomain format differs from default TabularDataset domain
        domain = horizontal_datasets[0].domain

        # Cache client answers
        # Dict[cl: int] maps queries to size/score for budget annealing
        candidates = self._compile_workload(workload)
        self.workload_closure = candidates.copy()
        client_answers = self._compute_client_answers(
            horizontal_datasets,
            candidates,
            original_dataset,
            save_cache=self.save_client_answer_cache,
            load_cache=self.load_client_answer_cache,
        )

        # Required for DistAIM, appends client IDs to answers
        if len(horizontal_datasets) > 1:
            client_answers = [
                (i, client_answers[i]) for i in range(0, len(horizontal_datasets))
            ]

        # Compute number of one-way marginal rounds (for initialisation) + number of AIM rounds
        self.num_oneways = len([cl for cl in candidates if len(cl) == 1])
        self.rounds = self._get_initial_num_rounds(len(domain))

        # Initialise privacy noise
        gauss_sigma, exp_epsilon = self._initialise_noise_params(self.rounds)
        self.init_gauss_sigma, self.init_exp_epsilon = gauss_sigma, exp_epsilon
        logger.debug(
            f"Initialised noise - sigma={self.init_gauss_sigma}, exp_eps={self.init_exp_epsilon}"
        )

        # Measure all one-way queries in downward closure of workload to initialise PGM
        model, measurements, engine = self._initialise_model(
            original_dataset, candidates, client_answers, gauss_sigma
        )

        terminate, t = False, 0
        while not terminate:
            t += 1

            # Use up whatever remaining budget there is for one last round
            # If not much budget is left, use remaining on this single (final) round
            if self._final_round(gauss_sigma, exp_epsilon, t):
                gauss_sigma, exp_epsilon = self._calibrate_remaining_budget(
                    gauss_sigma, exp_epsilon, t
                )
                terminate = True

            # Filter marginals based on size limit (determined by current budget spent)
            size_limit = self._get_size_limit(t)
            logger.debug(
                f"Current model size - {self._hypothetical_model_size(model.domain, model.cliques)}, Size limit - {size_limit}"
            )
            viable_candidates = self._filter_candidates(candidates, model, size_limit)
            # Perform AIM step - select query, measure marginal, re-estimate model
            (
                model,
                prev_est,
                new_est,
                query_selected,
                measurements,
                round_sample_size,
            ) = self._aim_step(
                engine,
                model,
                measurements,
                client_answers,
                viable_candidates,
                exp_epsilon,
                gauss_sigma,
                t,
            )

            # Calculate query error and error thresh to decide whether to anneal budget
            query_error = np.linalg.norm(new_est - prev_est, 1)
            logger.debug(f"Query error change - {query_selected} = {query_error}")
            error_thresh = (
                (self.n * gauss_sigma / round_sample_size)
                * np.sqrt(2 / np.pi)
                * domain.size(query_selected)
            )
            logger.debug(f"Anneal condition - {query_error} <= {error_thresh}")
            if query_error <= error_thresh:  # if error is bad, anneal budget
                gauss_sigma, exp_epsilon = self._anneal_budget(gauss_sigma, exp_epsilon)
                logger.debug(f"Budget annealed - {gauss_sigma}, {exp_epsilon}")

        self.internal_metrics["actual_rounds"] = t

        self.model = model
        if self.backend_model == "rap":
            synth = engine.synthetic_data(rows=self.synth_rows)
            synth = [None, synth.df]  # match aim_results format
        else:
            engine.iters = self.pgm_final_iters
            model = engine.estimate(measurements, total=self.n)
            synth = model.synthetic_data(rows=self.synth_rows)
        self._update_model_size(model.size)
        for m in measurements:
            logger.debug(f"{m[3], m[2]}")
        logger.debug(f"Accountant Eps: {self.accountant.get_eps()}")
        return synth

    def _format_workload(self, workload: np.array) -> List[Tuple[np.array, int]]:
        # TODO: Change AIM to only take w instead of (w, 1.0)
        return [(w, 1.0) for w in workload]

    def generate(
        self, dataset: TabularDataset, workload=None, **kwargs
    ) -> TabularDataset:
        assert workload is not None
        self.n = dataset.n
        self.target = dataset.y
        # workload = self._format_workload(workload)
        aim_results = self._generate([dataset], workload)
        return TabularDataset(
            f"{self.name} {dataset.name}", None, aim_results[1], dataset.domain
        )
